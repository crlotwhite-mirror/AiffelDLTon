{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "774dd405",
   "metadata": {},
   "source": [
    "### 머신러닝 MultinomialNB test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6804514",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T17:07:58.340351600Z",
     "start_time": "2023-07-09T17:07:56.251535200Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB #다항분포 나이브 베이즈 모델\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from konlpy.tag import Mecab, Okt, Kkma\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding, Dropout\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from collections import Counter\n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "try:\n",
    "    from koeda import AEDA, EDA, RD, RI, SR, RS\n",
    "except ImportError:\n",
    "    !pip install koeda\n",
    "    from koeda import AEDA, EDA, RD, RI, SR, RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4346682",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T17:07:58.869212100Z",
     "start_time": "2023-07-09T17:07:58.854447900Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    def _aug_setup():\n",
    "        global config\n",
    "\n",
    "        if config['aug']['mode'] == 'e':\n",
    "            augmenter = EDA(morpheme_analyzer=config['morp'], alpha_sr=config['aug']['sr']['a'], alpha_ri=config['aug']['ri']['a'], alpha_rs=config['aug']['rs']['a'], prob_rd=config['aug']['rd']['a'])\n",
    "            p = (config['aug']['sr']['p'], config['aug']['ri']['p'], config['aug']['rs']['p'], config['aug']['rd']['p'])\n",
    "        elif config['aug']['mode'] == 'a':\n",
    "            augmenter = AEDA(morpheme_analyzer=config['morp'], punc_ratio=0.3)\n",
    "            p = max(config['aug']['sr']['p'], config['aug']['ri']['p'], config['aug']['rs']['p'], config['aug']['rd']['p'])\n",
    "        else:\n",
    "            augmenter = []\n",
    "            if config['aug']['rd']['p'] != 0.0:\n",
    "                augmenter.append(\n",
    "                    (RD(morpheme_analyzer=config['morp']), config['aug']['rd']['p'])\n",
    "                )\n",
    "\n",
    "            if config['aug']['ri']['p'] != 0.0:\n",
    "                augmenter.append(\n",
    "                    (RI(morpheme_analyzer=config['morp'], stopword=config['aug']['stopword']), config['aug']['ri']['p'])\n",
    "                )\n",
    "\n",
    "            if config['aug']['sr']['p'] != 0.0:\n",
    "                augmenter.append(\n",
    "                    (SR(morpheme_analyzer=config['morp'], stopword=config['aug']['stopword']), config['aug']['sr']['p'])\n",
    "                )\n",
    "\n",
    "            if config['aug']['rs']['p'] != 0.0:\n",
    "                augmenter.append(\n",
    "                    (RS(morpheme_analyzer=config['morp']), config['aug']['rs']['p'])\n",
    "                )\n",
    "            p = None\n",
    "\n",
    "        return augmenter, p\n",
    "\n",
    "    def _aug(text):\n",
    "        global config\n",
    "        nonlocal augmenter\n",
    "        nonlocal p\n",
    "\n",
    "        if isinstance(augmenter, list):\n",
    "            result = text\n",
    "\n",
    "            for aug, p in augmenter:\n",
    "                result = aug(result, p, config['aug']['repetition'])\n",
    "        else:\n",
    "            result = augmenter(text, p, config['aug']['repetition'])\n",
    "\n",
    "        return result\n",
    "    def _tokenize(text):\n",
    "        result=[]\n",
    "        tokenlist = config['morp'].pos(text)#,flatten=False \n",
    "        for word in tokenlist:\n",
    "            result.append(word[0]+'__'+word[1].lower()) \n",
    "        return result\n",
    "    \n",
    "    \n",
    "    global config\n",
    "\n",
    "    if config['is_cut']:\n",
    "        # 'conversation' 열의 각 항목에 대한 문자 수를 계산합니다.\n",
    "        data['conversation_length'] = data['conversation'].apply(len)\n",
    "\n",
    "        # 문자 수가 400 미만인 행만 선택합니다.\n",
    "        data = data[data['conversation_length'] < config['cut_point']]\n",
    "\n",
    "    if config['is_aug']:\n",
    "        # 중복 augmenter 생성 방지를 위해서 처음 한번에 생성\n",
    "        augmenter, p = _aug_setup()\n",
    "        \n",
    "        # 랜덤하게 행 선택 (예: 전체 행의 20%를 선택)\n",
    "        random_indices = np.random.choice(data.index, size=int(len(data) * config['aug']['ratio']), replace=False)\n",
    "\n",
    "        # 선택된 행에 대해 Random swap 함수 적용\n",
    "        augmented_rows = data.loc[random_indices, 'conversation'].apply(_aug)\n",
    "\n",
    "        # 증강된 데이터를 복사하고, 'text' 열에 증강된 텍스트를 삽입\n",
    "        new_rows = data.loc[random_indices].copy()\n",
    "        new_rows['conversation'] = augmented_rows\n",
    "#         print(new_rows['conversation'])\n",
    "        # 형태소 분석 실행\n",
    "#         data['conversation'] = data['conversation'].apply(config['morp'].morphs)\n",
    "\n",
    "        # 원본 데이터프레임에 증강된 데이터 추가\n",
    "        data = pd.concat([data, new_rows])\n",
    "        data['conversation'] = data['conversation'].apply(_tokenize)\n",
    "    else:\n",
    "        \n",
    "        data['conversation'] = data['conversation'].apply(_tokenize)\n",
    "\n",
    "    if config['is_stopword']:\n",
    "        documents_by_class = df.groupby('class')['conversation'].apply(list)\n",
    "        cls_nm = documents_by_class.index.tolist()\n",
    "        cls_cnt = []\n",
    "        for class_name, documents in documents_by_class.items():\n",
    "            result = []\n",
    "            for content in documents :\n",
    "                result.extend(content)\n",
    "            cls_cnt.append(result)\n",
    "        total_counts = Counter('')\n",
    "        temp = []\n",
    "        for i in range(len(cls_nm)):\n",
    "            word_counts = Counter(cls_cnt[i])\n",
    "            total_counts = total_counts + word_counts\n",
    "            temp.append(word_counts)\n",
    "        temp2 =[]\n",
    "        for i in range(len(cls_nm)):\n",
    "            word_ratios = {word: count / total_counts[word] if word in total_counts else 1 for word, count in temp[i].items() if count > 10}\n",
    "            sorted_word_ratios = sorted(word_ratios.items(), key=lambda x: x[1], reverse=True)\n",
    "        #     sorted_word_ratios = sorted(word_ratios.items(), key=lambda x: x[1])\n",
    "            temp2.append(sorted_word_ratios)\n",
    "        word_counts_sorted = dict(sorted(total_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "        stopword = []\n",
    "        for word, count in word_counts_sorted.items():\n",
    "            if count < 50 : \n",
    "                break\n",
    "            flag = True\n",
    "            for tt in range(len(temp2)) :\n",
    "                for i in range(len(temp2[tt])) :\n",
    "                    if temp2[tt][i][1]<=0.4 :\n",
    "                        break\n",
    "                    #threshHold보다 큰 비율을 가지고 있는 단어는 불용어에서 제외한다\n",
    "                    if word==temp2[tt][i][0] and temp2[tt][i][1]>0.4 :\n",
    "                        flag = False\n",
    "    #                     print(cls_nm[tt])\n",
    "    #                     print(word)#불용어 제외 단어\n",
    "                        break\n",
    "                if not flag :\n",
    "                    break\n",
    "            if flag :\n",
    "                stopword.append(word)\n",
    "        df['conversation'] = df['conversation'].apply(lambda x: [word for word in x if word not in stopword])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf195dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def nfold_cross_validation(X, y, n_splits, model):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        score = model.score(X_test, y_test)\n",
    "        scores.append(score)\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3b6ce50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T17:17:36.270882100Z",
     "start_time": "2023-07-09T17:17:36.261419600Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'is_preprocess': True, # 전처리 여부\n",
    "    'is_dnn': True, # 딥러닝 사용 여부\n",
    "    'morp': Mecab(), # 형태소 분석기 변경,Kkma()\n",
    "    'is_aug': True, # 데이터 augment 사용 여부\n",
    "    # 데이터 augmentation 파라미터의 경우 아래의 글들을 참고하면 좋음\n",
    "    # https://yeon22.tistory.com/203\n",
    "    # https://catsirup.github.io/ai/2020/04/21/nlp_data_argumentation.html\n",
    "    'aug': {\n",
    "        'ratio': 0.3, # 적용할 데이터의 비율\n",
    "        # 사용하기 싫은 것은 p의 값을 0.0으로 지정 한다.\n",
    "        'rd': {\n",
    "            # 여기서 a는 사실 확률이다.\n",
    "            # API 통일을 위해 a로 표기했지만, prob_rd가 본명칭\n",
    "            'a': 0.3,\n",
    "            'p': 0.4,\n",
    "        }, # RandomDeletion\n",
    "        'ri': {\n",
    "            'a': 0.3, # alpha 값이고 데이터 증강 기법의 강도를 의미한다.\n",
    "            'p': 0.4, # p는 증강 기법이 \"얼마나 자주\" 적용될지\n",
    "        }, # RandomInsertion\n",
    "        'sr': {\n",
    "            'a': 0.3,\n",
    "            'p': 0.4,\n",
    "        }, # SynonymReplacement\n",
    "        'rs': {\n",
    "            'a': 0.3,\n",
    "            'p': 0.4,\n",
    "        }, # RandomSwap\n",
    "        'mode': 'e', # EDA: e, AEDA: a, other: o\n",
    "        'stopword': True,\n",
    "        'repetition': 1 # 반복 여부인 것 같아요.\n",
    "    },\n",
    "    'is_cut': True, # 단어길이 자를건지\n",
    "    'cut_point': 400, # 자르는 기준\n",
    "    'is_word2vec': False, # word2vec 사용 여부\n",
    "    'is_fasttext': True, # fasttext 사용 여부\n",
    "    'is_stopword': True # syh님\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60f8244e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T17:30:46.229613400Z",
     "start_time": "2023-07-09T17:17:39.573110900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "get data\n",
      "preprocess\n",
      "start train\n",
      "select dnn\n",
      "tokenize\n",
      "split dataset\n",
      "283\n",
      "['지금__mag', '너__np', '스스로__nng', '를__jko', '죽여__vv+ec', '달__vx', '라고__ec', '하__xsv', '는__etm', '것__nnb', '인가__vcp+ef', '?__sf', '아닙니다__vcn+ef', '.__sf', '죄송__xr', '합니다__xsa+ef', '.__sf', '죽__vv', '을__etm', '거__nnb', '면__vcp+ec', '혼자__mag', '죽__vv', '지__ec', '우리__np', '까지__jx', '사건__nng', '에__jkb', '게__ec', '해__vx+ef', '?__sf', '진짜__mag', '죽여__vv+ec', '버리__vx', '고__ec', '싶__vx', '게__ec', '.__sf', '정말__mag', '잘못__mag', '했__vv+ep', '습니다__ef', '.__sf', '너__np', '가__jks', '선택__nng', '해__xsv+ef', '.__sf', '너__np', '가__jks', '죽__vv', '을래__ec', '네__mm', '가족__nng', '을__jko', '죽여__vv+ec', '줄까__vx+ef', '.__sf', '죄송__xr', '합니다__xsa+ef', '.__sf', '정말__mag', '잘못__mag', '했__vv+ep', '습니다__ef', '.__sf', '너__np', '에게__jkb', '는__jx', '선택__nng', '권__xsn', '이__jks', '없__va', '어__ef', '.__sf', '선택__nng', '못__mag', '한다면__vv+ec', '너__np', '와__jkb', '네__mm', '가족__nng', '까지__jx', '모조리__mag', '죽여__vv+ec', '버릴__vx+etm', '거__nnb', '야__vcp+ef', '.__sf', '선택__nng', '못__mag', '하__vv', '겠__ep', '습니다__ef', '.__sf', '한__mm', '번__nnbc', '만__jx', '도와__vv+ec', '주__vx', '세요__ep+ef', '.__sf', '그냥__mag', '다__mag', '죽여__vv+ec', '버려야__vx+ec+vx', '겠__ep', '군__ef', '.__sf', '이__np', '의__jkg', '없__va', '지__ef', '?__sf', '제발__mag', '도와__vv+ec', '주__vx', '세요__ep+ef', '.__sf']\n",
      "select fasttext\n",
      "create model\n",
      "model fit\n",
      "Epoch 1/30\n",
      "121/121 [==============================] - 2s 5ms/step - loss: 1.3570 - accuracy: 0.3471 - val_loss: 1.0941 - val_accuracy: 0.6618\n",
      "Epoch 2/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 1.0039 - accuracy: 0.5823 - val_loss: 0.7835 - val_accuracy: 0.7313\n",
      "Epoch 3/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.6913 - accuracy: 0.7323 - val_loss: 0.5991 - val_accuracy: 0.7780\n",
      "Epoch 4/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.5113 - accuracy: 0.8105 - val_loss: 0.5570 - val_accuracy: 0.7925\n",
      "Epoch 5/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.3936 - accuracy: 0.8557 - val_loss: 0.5516 - val_accuracy: 0.7936\n",
      "Epoch 6/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.3003 - accuracy: 0.8938 - val_loss: 0.5708 - val_accuracy: 0.8091\n",
      "Epoch 7/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.2560 - accuracy: 0.9143 - val_loss: 0.5371 - val_accuracy: 0.8205\n",
      "Epoch 8/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.2048 - accuracy: 0.9263 - val_loss: 0.6131 - val_accuracy: 0.8143\n",
      "Epoch 9/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.1756 - accuracy: 0.9387 - val_loss: 0.6488 - val_accuracy: 0.8164\n",
      "Epoch 10/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.1385 - accuracy: 0.9481 - val_loss: 0.6526 - val_accuracy: 0.8288\n",
      "Epoch 11/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.1563 - accuracy: 0.9460 - val_loss: 0.6046 - val_accuracy: 0.8257\n",
      "Epoch 12/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.1337 - accuracy: 0.9538 - val_loss: 0.6016 - val_accuracy: 0.8268\n",
      "Epoch 13/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.1202 - accuracy: 0.9624 - val_loss: 0.6543 - val_accuracy: 0.8257\n",
      "Epoch 14/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.1036 - accuracy: 0.9642 - val_loss: 0.6894 - val_accuracy: 0.8278\n",
      "Epoch 15/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.0854 - accuracy: 0.9701 - val_loss: 0.7652 - val_accuracy: 0.8237\n",
      "Epoch 16/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.0987 - accuracy: 0.9644 - val_loss: 0.6820 - val_accuracy: 0.8392\n",
      "Epoch 17/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.0838 - accuracy: 0.9717 - val_loss: 0.7003 - val_accuracy: 0.8382\n",
      "Epoch 18/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.0852 - accuracy: 0.9738 - val_loss: 0.7456 - val_accuracy: 0.8392\n",
      "Epoch 19/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.0719 - accuracy: 0.9753 - val_loss: 0.7352 - val_accuracy: 0.8392\n",
      "Epoch 20/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.0635 - accuracy: 0.9748 - val_loss: 0.8338 - val_accuracy: 0.8465\n",
      "Epoch 21/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.0728 - accuracy: 0.9722 - val_loss: 0.8040 - val_accuracy: 0.8475\n",
      "Epoch 22/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.0624 - accuracy: 0.9766 - val_loss: 0.7896 - val_accuracy: 0.8475\n",
      "Epoch 23/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.0658 - accuracy: 0.9774 - val_loss: 0.8362 - val_accuracy: 0.8434\n",
      "Epoch 24/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.0526 - accuracy: 0.9823 - val_loss: 0.9050 - val_accuracy: 0.8485\n",
      "Epoch 25/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.0451 - accuracy: 0.9818 - val_loss: 0.9402 - val_accuracy: 0.8361\n",
      "Epoch 26/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.0640 - accuracy: 0.9795 - val_loss: 0.9288 - val_accuracy: 0.8288\n",
      "Epoch 27/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.0372 - accuracy: 0.9844 - val_loss: 0.8715 - val_accuracy: 0.8434\n",
      "Epoch 28/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.0571 - accuracy: 0.9857 - val_loss: 0.9569 - val_accuracy: 0.8309\n",
      "Epoch 29/30\n",
      "121/121 [==============================] - 1s 4ms/step - loss: 0.0699 - accuracy: 0.9805 - val_loss: 0.9263 - val_accuracy: 0.8268\n",
      "Epoch 30/30\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.0636 - accuracy: 0.9800 - val_loss: 0.9192 - val_accuracy: 0.8382\n",
      "model evaluate\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.9192 - accuracy: 0.8382\n",
      "[0.919184684753418, 0.8381742835044861]\n"
     ]
    }
   ],
   "source": [
    "print('start')\n",
    "# 데이터는 그대로고 모델만 수정해서 확인할 경우\n",
    "# 중복으로 읽고 전처리하는 대신 FIXED 변수를 통해 제어하세요.\n",
    "# 'df' in globals() 은 변수의 존재 여부를 판단합니다.\n",
    "FIXED = False\n",
    "if not FIXED and 'df' not in globals():\n",
    "    print('get data')\n",
    "    df = pd.read_csv('./data/train.csv')\n",
    "\n",
    "    if config['is_preprocess']:\n",
    "        print('preprocess')\n",
    "        df = preprocess(df)\n",
    "else:\n",
    "    print('data was fixed')\n",
    "\n",
    "print('start train')\n",
    "if config['is_dnn']:\n",
    "    print('select dnn')\n",
    "    # 가정: 입력 크기는 1000, 출력 클래스는 2\n",
    "    max_words = 10000\n",
    "    output_dim = 4\n",
    "\n",
    "    print('tokenize')\n",
    "    # Tokenizer를 생성하고 텍스트 데이터에 적합시킵니다.\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(df['conversation'])\n",
    "\n",
    "    # 텍스트를 정수 인덱스 시퀀스로 변환합니다.\n",
    "    sequences = tokenizer.texts_to_sequences(df['conversation'])\n",
    "\n",
    "    # 시퀀스의 길이를 맞추기 위해 패딩을 추가합니다.\n",
    "    data_pad = pad_sequences(sequences, padding='pre')\n",
    "\n",
    "    # class 열을 숫자로 변환\n",
    "    encoder = LabelEncoder()\n",
    "    df['class'] = encoder.fit_transform(df['class'])\n",
    "\n",
    "    print('split dataset')\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_pad, df['class'], test_size=0.2)\n",
    "    print(len(data_pad[0]))\n",
    "    # word2vec\n",
    "    assert not (config['is_word2vec'] and config['is_fasttext']), 'you have to choose only one among word2vec and fasttext'\n",
    "    if config['is_word2vec'] or config['is_fasttext']:\n",
    "        word_index = tokenizer.word_index\n",
    "        index_word = {idx: word for word, idx in word_index.items()}\n",
    "        recovered = [[index_word[idx] for idx in seq if idx != 0] for seq in sequences]\n",
    "        print(recovered[0])\n",
    "        if config['is_word2vec']:\n",
    "            print('select word2vec')\n",
    "            # Word2Vec 모델 학습\n",
    "            word2vec_model = Word2Vec(sentences=recovered, \n",
    "                                    vector_size=100, \n",
    "                                    window=5, \n",
    "                                    min_count=1, \n",
    "#                                     workers=4,\n",
    "                                    sg=0)\n",
    "\n",
    "            # 단어 인덱스와 임베딩 매트릭스 생성\n",
    "#             vocab_size = len(word2vec_model.wv.vocab) + 1  # +1 for padding\n",
    "            vocab_size = len(word2vec_model.wv.key_to_index) + 1  \n",
    "            print(vocab_size)\n",
    "            embedding_dim = word2vec_model.wv.vector_size\n",
    "            print(embedding_dim)\n",
    "            embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "#             for i, word in enumerate(word2vec_model.wv.vocab):\n",
    "#                 embedding_matrix[i] = word2vec_model.wv[word]\n",
    "            for i in range(4,vocab_size):\n",
    "                if index_word[i] in word2vec_model.wv:\n",
    "                    embedding_matrix[i] = word2vec_model.wv[index_word[i]]\n",
    "            # Embedding layer with pre-trained Word2Vec weights\n",
    "            embedding_layer = Embedding(vocab_size, \n",
    "                                        embedding_dim, \n",
    "                                        weights=[embedding_matrix], \n",
    "                                        input_length=len(data_pad[0]),  # 뭐 차원이 다르다 이러면 요거 건드려 보세요.\n",
    "                                        trainable=True)  # Keep embeddings fixed\n",
    "        elif config['is_fasttext']:\n",
    "            print('select fasttext')\n",
    "            fasttext_model = FastText(\n",
    "                sentences=recovered,\n",
    "                window=5,\n",
    "                min_count=5,\n",
    "                workers=4,\n",
    "                sg=1\n",
    "            )\n",
    "\n",
    "            embedding_dim = fasttext_model.vector_size\n",
    "#             vocab_size = len(fasttext_model.wv.vocab) + 1\n",
    "            vocab_size = len(fasttext_model.wv.key_to_index) + 1  \n",
    "            embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "#             for i, word in enumerate(fasttext_model.wv.vocab):\n",
    "#                 embedding_matrix[i] = fasttext_model.wv[word]\n",
    "            for i in range(4,vocab_size):\n",
    "                if index_word[i] in fasttext_model.wv:\n",
    "                    embedding_matrix[i] = fasttext_model.wv[index_word[i]]\n",
    "            embedding_layer = Embedding(vocab_size,\n",
    "                                        embedding_dim,\n",
    "                                        weights=[embedding_matrix],\n",
    "                                        input_length=len(data_pad[0]),\n",
    "                                        trainable=True)\n",
    "\n",
    "    else:\n",
    "        print('select normal')\n",
    "        embedding_layer = Embedding(len(tokenizer.word_index)+1, 128, input_length=data_pad.shape[1])\n",
    "\n",
    "    print('create model')\n",
    "    # 모델을 생성합니다.\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))  # 드롭아웃 레이어 추가 (0.5는 드롭아웃 비율)\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))  # 드롭아웃 레이어 추가 (0.5는 드롭아웃 비율)\n",
    "    model.add(Dense(len(df['class'].unique()), activation='softmax'))\n",
    "\n",
    "    # 모델을 컴파일합니다.\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print('model fit')\n",
    "    # 모델을 학습합니다.\n",
    "    model.fit(x_train, y_train, epochs=30,validation_data=(x_test, y_test))\n",
    "\n",
    "    print('model evaluate')\n",
    "    temp = model.evaluate(x=x_test, y=y_test)\n",
    "    print(temp)\n",
    "#     model.evaluate(x=x_test, y=y_test)\n",
    "else:\n",
    "    print('select ml')\n",
    "    df['conversation'] = df['conversation'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    x_train = vectorizer.fit_transform(df['conversation'])\n",
    "    y_train = df['class']\n",
    "#     print(x_train[4777])\n",
    "#     print(len(y_train))\n",
    "#     assert len(x_train) == len(y_train)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2)\n",
    "\n",
    "\n",
    "    model = MultinomialNB()\n",
    "#     score = nfold_cross_validation(x_train,y_train,10,model)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    score = model.score(x_test, y_test)\n",
    "    print('Accuracy:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ba5b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterrableFit():\n",
    "    print('start train')\n",
    "    if config['is_dnn']:\n",
    "        print('select dnn')\n",
    "        # 가정: 입력 크기는 1000, 출력 클래스는 2\n",
    "        max_words = 10000\n",
    "        output_dim = 4\n",
    "\n",
    "        print('tokenize')\n",
    "        # Tokenizer를 생성하고 텍스트 데이터에 적합시킵니다.\n",
    "        tokenizer = Tokenizer(num_words=max_words)\n",
    "        tokenizer.fit_on_texts(df['conversation'])\n",
    "\n",
    "        # 텍스트를 정수 인덱스 시퀀스로 변환합니다.\n",
    "        sequences = tokenizer.texts_to_sequences(df['conversation'])\n",
    "\n",
    "        # 시퀀스의 길이를 맞추기 위해 패딩을 추가합니다.\n",
    "        data_pad = pad_sequences(sequences, padding='pre')\n",
    "\n",
    "        # class 열을 숫자로 변환\n",
    "        encoder = LabelEncoder()\n",
    "        df['class'] = encoder.fit_transform(df['class'])\n",
    "\n",
    "        print('split dataset')\n",
    "        x_train, x_test, y_train, y_test = train_test_split(data_pad, df['class'], test_size=0.2)\n",
    "        print(len(data_pad[0]))\n",
    "        # word2vec\n",
    "        assert not (config['is_word2vec'] and config['is_fasttext']), 'you have to choose only one among word2vec and fasttext'\n",
    "        if config['is_word2vec'] or config['is_fasttext']:\n",
    "            word_index = tokenizer.word_index\n",
    "            index_word = {idx: word for word, idx in word_index.items()}\n",
    "            recovered = [[index_word[idx] for idx in seq if idx != 0] for seq in sequences]\n",
    "            print(recovered[0])\n",
    "            if config['is_word2vec']:\n",
    "                print('select word2vec')\n",
    "                # Word2Vec 모델 학습\n",
    "                word2vec_model = Word2Vec(sentences=recovered, \n",
    "                                        vector_size=100, \n",
    "                                        window=5, \n",
    "                                        min_count=1, \n",
    "    #                                     workers=4,\n",
    "                                        sg=0)\n",
    "\n",
    "                # 단어 인덱스와 임베딩 매트릭스 생성\n",
    "    #             vocab_size = len(word2vec_model.wv.vocab) + 1  # +1 for padding\n",
    "                vocab_size = len(word2vec_model.wv.key_to_index) + 1  \n",
    "                print(vocab_size)\n",
    "                embedding_dim = word2vec_model.wv.vector_size\n",
    "                print(embedding_dim)\n",
    "                embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    #             for i, word in enumerate(word2vec_model.wv.vocab):\n",
    "    #                 embedding_matrix[i] = word2vec_model.wv[word]\n",
    "                for i in range(4,vocab_size):\n",
    "                    if index_word[i] in word2vec_model.wv:\n",
    "                        embedding_matrix[i] = word2vec_model.wv[index_word[i]]\n",
    "                # Embedding layer with pre-trained Word2Vec weights\n",
    "                embedding_layer = Embedding(vocab_size, \n",
    "                                            embedding_dim, \n",
    "                                            weights=[embedding_matrix], \n",
    "                                            input_length=len(data_pad[0]),  # 뭐 차원이 다르다 이러면 요거 건드려 보세요.\n",
    "                                            trainable=True)  # Keep embeddings fixed\n",
    "            elif config['is_fasttext']:\n",
    "                print('select fasttext')\n",
    "                fasttext_model = FastText(\n",
    "                    sentences=recovered,\n",
    "                    window=5,\n",
    "                    min_count=5,\n",
    "                    workers=4,\n",
    "                    sg=1\n",
    "                )\n",
    "\n",
    "                embedding_dim = fasttext_model.vector_size\n",
    "    #             vocab_size = len(fasttext_model.wv.vocab) + 1\n",
    "                vocab_size = len(fasttext_model.wv.key_to_index) + 1  \n",
    "                embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    #             for i, word in enumerate(fasttext_model.wv.vocab):\n",
    "    #                 embedding_matrix[i] = fasttext_model.wv[word]\n",
    "                for i in range(4,vocab_size):\n",
    "                    if index_word[i] in fasttext_model.wv:\n",
    "                        embedding_matrix[i] = fasttext_model.wv[index_word[i]]\n",
    "                embedding_layer = Embedding(vocab_size,\n",
    "                                            embedding_dim,\n",
    "                                            weights=[embedding_matrix],\n",
    "                                            input_length=len(data_pad[0]),\n",
    "                                            trainable=True)\n",
    "\n",
    "        else:\n",
    "            print('select normal')\n",
    "            embedding_layer = Embedding(len(tokenizer.word_index)+1, 128, input_length=data_pad.shape[1])\n",
    "\n",
    "        print('create model')\n",
    "        # 모델을 생성합니다.\n",
    "        model = Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(0.5))  # 드롭아웃 레이어 추가 (0.5는 드롭아웃 비율)\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(0.5))  # 드롭아웃 레이어 추가 (0.5는 드롭아웃 비율)\n",
    "        model.add(Dense(len(df['class'].unique()), activation='softmax'))\n",
    "\n",
    "        # 모델을 컴파일합니다.\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        print('model fit')\n",
    "        # 모델을 학습합니다.\n",
    "        model.fit(x_train, y_train, epochs=10,validation_data=(x_test, y_test))\n",
    "\n",
    "        print('model evaluate')\n",
    "        temp = model.evaluate(x=x_test, y=y_test)\n",
    "#         print(temp)\n",
    "        return temp[1]\n",
    "    else:\n",
    "        print('select ml')\n",
    "        x_train = df['conversation'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "        vectorizer = CountVectorizer()\n",
    "\n",
    "        x_train = vectorizer.fit_transform(x_train)\n",
    "        y_train = df['class']\n",
    "    #     print(x_train[4777])\n",
    "    #     print(len(y_train))\n",
    "    #     assert len(x_train) == len(y_train)\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2)\n",
    "\n",
    "\n",
    "        model = MultinomialNB()\n",
    "    #     score = nfold_cross_validation(x_train,y_train,10,model)\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        score = model.score(x_test, y_test)\n",
    "        print('Accuracy:', score)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af68c2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train\n",
      "select ml\n",
      "Accuracy: 0.8869294605809128\n",
      "start train\n",
      "select dnn\n",
      "tokenize\n",
      "split dataset\n",
      "323\n",
      "select normal\n",
      "create model\n",
      "model fit\n",
      "Epoch 1/10\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 1.4138 - accuracy: 0.2817 - val_loss: 1.3450 - val_accuracy: 0.4346\n",
      "Epoch 2/10\n",
      "121/121 [==============================] - 1s 5ms/step - loss: 0.9164 - accuracy: 0.6433 - val_loss: 0.5610 - val_accuracy: 0.7956\n",
      "Epoch 3/10\n",
      "121/121 [==============================] - 1s 5ms/step - loss: 0.2175 - accuracy: 0.9374 - val_loss: 0.4733 - val_accuracy: 0.8330\n",
      "Epoch 4/10\n",
      "121/121 [==============================] - 1s 5ms/step - loss: 0.0565 - accuracy: 0.9836 - val_loss: 0.5647 - val_accuracy: 0.8465\n",
      "Epoch 5/10\n",
      "121/121 [==============================] - 1s 5ms/step - loss: 0.0189 - accuracy: 0.9961 - val_loss: 0.6760 - val_accuracy: 0.8506\n",
      "Epoch 6/10\n",
      "121/121 [==============================] - 1s 5ms/step - loss: 0.0141 - accuracy: 0.9966 - val_loss: 0.6126 - val_accuracy: 0.8548\n",
      "Epoch 7/10\n",
      "121/121 [==============================] - 1s 5ms/step - loss: 0.0093 - accuracy: 0.9979 - val_loss: 0.6917 - val_accuracy: 0.8568\n",
      "Epoch 8/10\n",
      "121/121 [==============================] - 1s 5ms/step - loss: 0.0074 - accuracy: 0.9990 - val_loss: 0.7433 - val_accuracy: 0.8506\n",
      "Epoch 9/10\n",
      "121/121 [==============================] - 1s 5ms/step - loss: 0.0076 - accuracy: 0.9984 - val_loss: 0.8448 - val_accuracy: 0.8361\n",
      "Epoch 10/10\n",
      "121/121 [==============================] - 1s 5ms/step - loss: 0.0072 - accuracy: 0.9984 - val_loss: 0.7690 - val_accuracy: 0.8517\n",
      "model evaluate\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.7690 - accuracy: 0.8517\n",
      "start train\n",
      "select dnn\n",
      "tokenize\n",
      "split dataset\n",
      "323\n",
      "['지금__mag', '너__np', '스스로__nng', '를__jko', '죽이__vv', '어__ecs', '달라__vv', '고__ece', '애원__nng', '하__xsv', '는__etd', '것__nnb', '이__vcp', 'ㄴ가__efq', '?__sf', '아니__vcn', 'ㅂ니다__efn', '.__sf', '죄송__xr', '하__xsa', 'ㅂ니다__efn', '.__sf', '죽__vv', '을__etd', '거__nnb', '이__vcp', '면__ece', '혼자__nng', '죽__vv', '지__ecd', '우리__np', '까지__jx', '사건__nng', '에__jkm', '휘말리__vv', '게__ecd', '하__vv', '어__ecs', '?__sf', '진짜__mag', '죽이__vv', '어__ecs', '버리__vxv', '고__ece', '싶__vxa', '게__ecd', '.__sf', '정말__mag', '잘못하__vv', '었__ept', '습니다__efn', '.__sf', '너__np', '가__jks', '선택__nng', '하__xsv', '어__ecs', '.__sf', '너__np', '가__jks', '죽__vv', '을래__efn', '네__mdn', '가족__nng', '을__jko', '죽여주__vv', 'ㄹ까__efq', '.__sf', '죄송__xr', '하__xsa', 'ㅂ니다__efn', '.__sf', '정말__mag', '잘못하__vv', '었__ept', '습니다__efn', '.__sf', '너__np', '에게__jkm', '는__jx', '선택권__nng', '이__jks', '없__va', '어__ecd', '.__sf', '선택__nng', '못하__vx', 'ㄴ다면__ece', '너와__nng', '네__mdn', '가족__nng', '까지__jx', '모조리__mag', '죽이__vv', '어__ecs', '버리__vxv', 'ㄹ__etd', '거__nnb', '야__jx', '.__sf', '선택__nng', '못하__va', '겠__ept', '습니다__efn', '.__sf', '한번__nng', '만__jx', '도와주__vv', '세요__efn', '.__sf', '그냥__mag', '다__mag', '죽이__vv', '어__ecs', '버리__vxv', '어야__ecd', '겠군__un', '.__sf', '이의__nng', '없__va', '지__ecd', '?__sf', '저__np', '의__jkg', '발__nng', '돕__vv', '아__ecs', '주__vxv', '세요__efn', '.__sf']\n",
      "select word2vec\n",
      "10000\n",
      "100\n",
      "create model\n",
      "model fit\n",
      "Epoch 1/10\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 1.4172 - accuracy: 0.3515 - val_loss: 1.1516 - val_accuracy: 0.5187\n",
      "Epoch 2/10\n",
      "121/121 [==============================] - 1s 5ms/step - loss: 1.2007 - accuracy: 0.4896 - val_loss: 1.0057 - val_accuracy: 0.6141\n",
      "Epoch 3/10\n",
      "121/121 [==============================] - 1s 4ms/step - loss: 1.0063 - accuracy: 0.5810 - val_loss: 0.9042 - val_accuracy: 0.6432\n",
      "Epoch 4/10\n",
      "121/121 [==============================] - 1s 4ms/step - loss: 0.8933 - accuracy: 0.6537 - val_loss: 0.8638 - val_accuracy: 0.6649\n",
      "Epoch 5/10\n",
      "121/121 [==============================] - 1s 4ms/step - loss: 0.7278 - accuracy: 0.7196 - val_loss: 0.8579 - val_accuracy: 0.6618\n",
      "Epoch 6/10\n",
      "121/121 [==============================] - 1s 4ms/step - loss: 0.6965 - accuracy: 0.7536 - val_loss: 0.8400 - val_accuracy: 0.6763\n",
      "Epoch 7/10\n",
      "121/121 [==============================] - 1s 4ms/step - loss: 0.5714 - accuracy: 0.7850 - val_loss: 0.8358 - val_accuracy: 0.6795\n",
      "Epoch 8/10\n",
      "121/121 [==============================] - 1s 4ms/step - loss: 0.5232 - accuracy: 0.7978 - val_loss: 0.8441 - val_accuracy: 0.6846\n",
      "Epoch 9/10\n",
      "121/121 [==============================] - 1s 4ms/step - loss: 0.4799 - accuracy: 0.8206 - val_loss: 0.8374 - val_accuracy: 0.6961\n",
      "Epoch 10/10\n",
      "121/121 [==============================] - 1s 4ms/step - loss: 0.4505 - accuracy: 0.8320 - val_loss: 0.8548 - val_accuracy: 0.6909\n",
      "model evaluate\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.8548 - accuracy: 0.6909\n",
      "start train\n",
      "select dnn\n",
      "tokenize\n",
      "split dataset\n",
      "323\n",
      "['지금__mag', '너__np', '스스로__nng', '를__jko', '죽이__vv', '어__ecs', '달라__vv', '고__ece', '애원__nng', '하__xsv', '는__etd', '것__nnb', '이__vcp', 'ㄴ가__efq', '?__sf', '아니__vcn', 'ㅂ니다__efn', '.__sf', '죄송__xr', '하__xsa', 'ㅂ니다__efn', '.__sf', '죽__vv', '을__etd', '거__nnb', '이__vcp', '면__ece', '혼자__nng', '죽__vv', '지__ecd', '우리__np', '까지__jx', '사건__nng', '에__jkm', '휘말리__vv', '게__ecd', '하__vv', '어__ecs', '?__sf', '진짜__mag', '죽이__vv', '어__ecs', '버리__vxv', '고__ece', '싶__vxa', '게__ecd', '.__sf', '정말__mag', '잘못하__vv', '었__ept', '습니다__efn', '.__sf', '너__np', '가__jks', '선택__nng', '하__xsv', '어__ecs', '.__sf', '너__np', '가__jks', '죽__vv', '을래__efn', '네__mdn', '가족__nng', '을__jko', '죽여주__vv', 'ㄹ까__efq', '.__sf', '죄송__xr', '하__xsa', 'ㅂ니다__efn', '.__sf', '정말__mag', '잘못하__vv', '었__ept', '습니다__efn', '.__sf', '너__np', '에게__jkm', '는__jx', '선택권__nng', '이__jks', '없__va', '어__ecd', '.__sf', '선택__nng', '못하__vx', 'ㄴ다면__ece', '너와__nng', '네__mdn', '가족__nng', '까지__jx', '모조리__mag', '죽이__vv', '어__ecs', '버리__vxv', 'ㄹ__etd', '거__nnb', '야__jx', '.__sf', '선택__nng', '못하__va', '겠__ept', '습니다__efn', '.__sf', '한번__nng', '만__jx', '도와주__vv', '세요__efn', '.__sf', '그냥__mag', '다__mag', '죽이__vv', '어__ecs', '버리__vxv', '어야__ecd', '겠군__un', '.__sf', '이의__nng', '없__va', '지__ecd', '?__sf', '저__np', '의__jkg', '발__nng', '돕__vv', '아__ecs', '주__vxv', '세요__efn', '.__sf']\n",
      "select fasttext\n",
      "create model\n",
      "model fit\n",
      "Epoch 1/10\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 1.4463 - accuracy: 0.2700 - val_loss: 1.3806 - val_accuracy: 0.2541\n",
      "Epoch 2/10\n",
      "121/121 [==============================] - 1s 4ms/step - loss: 1.3753 - accuracy: 0.2900 - val_loss: 1.3305 - val_accuracy: 0.3112\n",
      "Epoch 3/10\n",
      "121/121 [==============================] - 1s 4ms/step - loss: 1.2879 - accuracy: 0.3904 - val_loss: 1.1634 - val_accuracy: 0.5384\n",
      "Epoch 4/10\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 1.1049 - accuracy: 0.4953 - val_loss: 0.9624 - val_accuracy: 0.6504\n",
      "Epoch 5/10\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.9275 - accuracy: 0.5729 - val_loss: 0.8499 - val_accuracy: 0.6878\n",
      "Epoch 6/10\n",
      "121/121 [==============================] - 1s 4ms/step - loss: 0.8153 - accuracy: 0.6327 - val_loss: 0.7661 - val_accuracy: 0.7137\n",
      "Epoch 7/10\n",
      "121/121 [==============================] - 1s 4ms/step - loss: 0.7352 - accuracy: 0.6633 - val_loss: 0.6855 - val_accuracy: 0.7365\n",
      "Epoch 8/10\n",
      "121/121 [==============================] - 1s 4ms/step - loss: 0.6564 - accuracy: 0.7020 - val_loss: 0.6535 - val_accuracy: 0.7614\n",
      "Epoch 9/10\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.5624 - accuracy: 0.7609 - val_loss: 0.6485 - val_accuracy: 0.7697\n",
      "Epoch 10/10\n",
      "121/121 [==============================] - 0s 4ms/step - loss: 0.4870 - accuracy: 0.8071 - val_loss: 0.5887 - val_accuracy: 0.7853\n",
      "model evaluate\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 0.5887 - accuracy: 0.7853\n"
     ]
    }
   ],
   "source": [
    "# iterArray[True,False]\n",
    "# config['is_aug'] = True\n",
    "# config['is_stopword']\n",
    "# config['is_cut']\n",
    "import itertools\n",
    "\n",
    "variables = [True, False]\n",
    "\n",
    "combinations = list(itertools.product(variables, repeat=3))\n",
    "resultScoreArr = []\n",
    "config['morp'] = Kkma()\n",
    "# config['morp'] = Mecab()\n",
    "for combination in combinations:\n",
    "    config['is_aug'],config['is_stopword'],config['is_cut'] = combination[0],combination[1],combination[2]\n",
    "    config['is_dnn'] = False\n",
    "    df = pd.read_csv('./data/train.csv')\n",
    "    df = preprocess(df)\n",
    "    score = iterrableFit()\n",
    "    temparr = [config['is_aug'],config['is_stopword'],config['is_cut'],config['is_dnn'],config['is_word2vec'],config['is_fasttext'],score]\n",
    "    resultScoreArr.append(temparr)\n",
    "    config['is_dnn'] = True\n",
    "    config['is_word2vec'] = False\n",
    "    config['is_fasttext'] = False\n",
    "    score = iterrableFit()\n",
    "    temparr = [config['is_aug'],config['is_stopword'],config['is_cut'],config['is_dnn'],config['is_word2vec'],config['is_fasttext'],score]\n",
    "    resultScoreArr.append(temparr)\n",
    "    config['is_word2vec'] = True\n",
    "    score = iterrableFit()\n",
    "    temparr = [config['is_aug'],config['is_stopword'],config['is_cut'],config['is_dnn'],config['is_word2vec'],config['is_fasttext'],score]\n",
    "    resultScoreArr.append(temparr)\n",
    "    config['is_word2vec'] = False\n",
    "    config['is_fasttext'] = True\n",
    "    score = iterrableFit()\n",
    "    temparr = [config['is_aug'],config['is_stopword'],config['is_cut'],config['is_dnn'],config['is_word2vec'],config['is_fasttext'],score]\n",
    "    resultScoreArr.append(temparr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a614232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[True, True, True, False, False, True, 0.8973029045643154],\n",
       " [True, True, True, True, False, False, 0.8734439611434937],\n",
       " [True, True, True, True, True, False, 0.7323651313781738],\n",
       " [True, True, True, True, False, True, 0.819502055644989],\n",
       " [True, True, False, False, False, True, 0.9006815968841285],\n",
       " [True, True, False, True, False, False, 0.8763388395309448],\n",
       " [True, True, False, True, True, False, 0.7332035303115845],\n",
       " [True, True, False, True, False, True, 0.8276533484458923],\n",
       " [True, False, True, False, False, True, 0.8869294605809128],\n",
       " [True, False, True, True, False, False, 0.8672199249267578],\n",
       " [True, False, True, True, True, False, 0.7105808854103088],\n",
       " [True, False, True, True, False, True, 0.8298755288124084],\n",
       " [True, False, False, False, False, True, 0.8958130477117819],\n",
       " [True, False, False, True, False, False, 0.8802337050437927],\n",
       " [True, False, False, True, True, False, 0.7185978293418884],\n",
       " [True, False, False, True, False, True, 0.82473224401474],\n",
       " [False, True, True, False, False, True, 0.865047233468286],\n",
       " [False, True, True, True, False, False, 0.8380566835403442],\n",
       " [False, True, True, True, True, False, 0.662618100643158],\n",
       " [False, True, True, True, False, True, 0.8029689788818359],\n",
       " [False, True, False, False, False, True, 0.8734177215189873],\n",
       " [False, True, False, True, False, False, 0.8734177350997925],\n",
       " [False, True, False, True, True, False, 0.8227847814559937],\n",
       " [False, True, False, True, False, True, 0.8379746675491333],\n",
       " [False, False, True, False, False, True, 0.8771929824561403],\n",
       " [False, False, True, True, False, False, 0.8434547781944275],\n",
       " [False, False, True, True, True, False, 0.6774628758430481],\n",
       " [False, False, True, True, False, True, 0.8016194105148315],\n",
       " [False, False, False, False, False, True, 0.859493670886076],\n",
       " [False, False, False, True, False, False, 0.8708860874176025],\n",
       " [False, False, False, True, True, False, 0.6911392211914062],\n",
       " [False, False, False, True, False, True, 0.798734188079834]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[config['is_aug'],config['is_stopword'],config['is_cut'],config['is_dnn'],config['is_word2vec'],config['is_fasttext'],score]\n",
    "resultScoreArr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f9c418",
   "metadata": {},
   "source": [
    "필요시 참고 링크\n",
    "KoEDA\n",
    "https://github.com/toriving/KoEDA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
