{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cddb3df",
   "metadata": {},
   "source": [
    "### 머신러닝 MultinomialNB test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f9b9719",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T17:07:58.340351600Z",
     "start_time": "2023-07-09T17:07:56.251535200Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB #다항분포 나이브 베이즈 모델\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from konlpy.tag import Mecab, Okt, Kkma\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding\n",
    "from gensim.models import Word2Vec, FastText\n",
    "try:\n",
    "    from koeda import AEDA, EDA, RD, RI, SR, RS\n",
    "except ImportError:\n",
    "    !pip install koeda\n",
    "    from koeda import AEDA, EDA, RD, RI, SR, RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T17:07:58.869212100Z",
     "start_time": "2023-07-09T17:07:58.854447900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    def _aug_setup():\n",
    "        global config\n",
    "\n",
    "        if config['aug']['mode'] == 'e':\n",
    "            augmenter = EDA(morpheme_analyzer=config['morp'], alpha_sr=config['aug']['sr']['a'], alpha_ri=config['aug']['ri']['a'], alpha_rs=config['aug']['rs']['a'], prob_rd=config['aug']['rd']['a'])\n",
    "            p = (config['aug']['sr']['p'], config['aug']['ri']['p'], config['aug']['rs']['p'], config['aug']['rd']['p'])\n",
    "        elif config['aug']['mode'] == 'a':\n",
    "            augmenter = AEDA(morpheme_analyzer=config['morp'], punc_ratio=0.3)\n",
    "            p = max(config['aug']['sr']['p'], config['aug']['ri']['p'], config['aug']['rs']['p'], config['aug']['rd']['p'])\n",
    "        else:\n",
    "            augmenter = []\n",
    "            if config['aug']['rd']['p'] != 0.0:\n",
    "                augmenter.append(\n",
    "                    (RD(morpheme_analyzer=config['morp']), config['aug']['rd']['p'])\n",
    "                )\n",
    "\n",
    "            if config['aug']['ri']['p'] != 0.0:\n",
    "                augmenter.append(\n",
    "                    (RI(morpheme_analyzer=config['morp'], stopword=config['aug']['stopword']), config['aug']['ri']['p'])\n",
    "                )\n",
    "\n",
    "            if config['aug']['sr']['p'] != 0.0:\n",
    "                augmenter.append(\n",
    "                    (SR(morpheme_analyzer=config['morp'], stopword=config['aug']['stopword']), config['aug']['sr']['p'])\n",
    "                )\n",
    "\n",
    "            if config['aug']['rs']['p'] != 0.0:\n",
    "                augmenter.append(\n",
    "                    (RS(morpheme_analyzer=config['morp']), config['aug']['rs']['p'])\n",
    "                )\n",
    "            p = None\n",
    "\n",
    "        return augmenter, p\n",
    "\n",
    "    def _aug(text):\n",
    "        global config\n",
    "        nonlocal augmenter\n",
    "        nonlocal p\n",
    "\n",
    "        if isinstance(augmenter, list):\n",
    "            result = text\n",
    "\n",
    "            for aug, p in augmenter:\n",
    "                result = aug(result, p, config['aug']['repetition'])\n",
    "        else:\n",
    "            result = augmenter(text, p, config['aug']['repetition'])\n",
    "\n",
    "        return result\n",
    "\n",
    "    global config\n",
    "\n",
    "    if config['is_cut']:\n",
    "        # 'conversation' 열의 각 항목에 대한 문자 수를 계산합니다.\n",
    "        data['conversation_length'] = data['conversation'].apply(len)\n",
    "\n",
    "        # 문자 수가 400 미만인 행만 선택합니다.\n",
    "        data = data[data['conversation_length'] < config['cut_point']]\n",
    "\n",
    "    if config['is_aug']:\n",
    "        # 중복 augmenter 생성 방지를 위해서 처음 한번에 생성\n",
    "        augmenter, p = _aug_setup()\n",
    "        \n",
    "        # 랜덤하게 행 선택 (예: 전체 행의 20%를 선택)\n",
    "        random_indices = np.random.choice(data.index, size=int(len(data) * config['aug']['ratio']), replace=False)\n",
    "\n",
    "        # 선택된 행에 대해 Random swap 함수 적용\n",
    "        augmented_rows = data.loc[random_indices, 'conversation'].apply(_aug)\n",
    "\n",
    "        # 증강된 데이터를 복사하고, 'text' 열에 증강된 텍스트를 삽입\n",
    "        new_rows = data.loc[random_indices].copy()\n",
    "        new_rows['conversation'] = augmented_rows\n",
    "        \n",
    "        # 형태소 분석 실행\n",
    "        data['conversation'] = data['conversation'].apply(config['morp'].morphs)\n",
    "\n",
    "        # 원본 데이터프레임에 증강된 데이터 추가\n",
    "        data = pd.concat([data, new_rows])\n",
    "    else:\n",
    "        data['conversation'] = data['conversation'].apply(config['morp'].morphs)\n",
    "\n",
    "    if config['is_stopword']:\n",
    "        pass\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T17:17:36.270882100Z",
     "start_time": "2023-07-09T17:17:36.261419600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'is_preprocess': True, # 전처리 여부\n",
    "    'is_dnn': True, # 딥러닝 사용 여부\n",
    "    'morp': Kkma(), # 형태소 분석기 변경,\n",
    "    'is_aug': False, # 데이터 augment 사용 여부\n",
    "    # 데이터 augmentation 파라미터의 경우 아래의 글들을 참고하면 좋음\n",
    "    # https://yeon22.tistory.com/203\n",
    "    # https://catsirup.github.io/ai/2020/04/21/nlp_data_argumentation.html\n",
    "    'aug': {\n",
    "        'ratio': 0.3, # 적용할 데이터의 비율\n",
    "        # 사용하기 싫은 것은 p의 값을 0.0으로 지정 한다.\n",
    "        'rd': {\n",
    "            # 여기서 a는 사실 확률이다.\n",
    "            # API 통일을 위해 a로 표기했지만, prob_rd가 본명칭\n",
    "            'a': 0.3,\n",
    "            'p': 0.4,\n",
    "        }, # RandomDeletion\n",
    "        'ri': {\n",
    "            'a': 0.3, # alpha 값이고 데이터 증강 기법의 강도를 의미한다.\n",
    "            'p': 0.4, # p는 증강 기법이 \"얼마나 자주\" 적용될지\n",
    "        }, # RandomInsertion\n",
    "        'sr': {\n",
    "            'a': 0.3,\n",
    "            'p': 0.4,\n",
    "        }, # SynonymReplacement\n",
    "        'rs': {\n",
    "            'a': 0.3,\n",
    "            'p': 0.4,\n",
    "        }, # RandomSwap\n",
    "        'mode': 'e', # EDA: e, AEDA: a, other: o\n",
    "        'stopword': True,\n",
    "        'repetition': 1 # 반복 여부인 것 같아요.\n",
    "    },\n",
    "    'is_cut': True, # 단어길이 자를건지\n",
    "    'cut_point': 400, # 자르는 기준\n",
    "    'is_word2vec': False, # word2vec 사용 여부\n",
    "    'is_fasttext': True, # fasttext 사용 여부\n",
    "    'is_stopword': False # syh님\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25221c0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T17:30:46.229613400Z",
     "start_time": "2023-07-09T17:17:39.573110900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "data was fixed\n",
      "start train\n",
      "select dnn\n",
      "tokenize\n",
      "split dataset\n",
      "select fasttext\n",
      "create model\n",
      "model fit\n",
      "Epoch 1/10\n",
      " 5/93 [>.............................] - ETA: 1s - loss: 2.8289 - accuracy: 0.3250 "
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential_3/embedding_3/embedding_lookup' defined at (most recent call last):\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 728, in start\n      self.io_loop.start()\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\tama0\\AppData\\Local\\Temp\\ipykernel_5304\\1304904449.py\", line 112, in <module>\n      model.fit(x_train, y_train, epochs=10)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1080, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\engine\\sequential.py\", line 405, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\engine\\functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\engine\\functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py\", line 272, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'sequential_3/embedding_3/embedding_lookup'\nindices[10,28] = 999 is not in [0, 999)\n\t [[{{node sequential_3/embedding_3/embedding_lookup}}]] [Op:__inference_train_function_7545]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 112\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmodel fit\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    111\u001b[0m \u001b[39m# 모델을 학습합니다.\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[0;32m    114\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmodel evaluate\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    115\u001b[0m model\u001b[39m.\u001b[39mevaluate(x\u001b[39m=\u001b[39mx_test, y\u001b[39m=\u001b[39my_test)\n",
      "File \u001b[1;32mc:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential_3/embedding_3/embedding_lookup' defined at (most recent call last):\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 728, in start\n      self.io_loop.start()\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\tama0\\AppData\\Local\\Temp\\ipykernel_5304\\1304904449.py\", line 112, in <module>\n      model.fit(x_train, y_train, epochs=10)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1080, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\engine\\sequential.py\", line 405, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\engine\\functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\engine\\functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\tama0\\anaconda3\\envs\\dlton210\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py\", line 272, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'sequential_3/embedding_3/embedding_lookup'\nindices[10,28] = 999 is not in [0, 999)\n\t [[{{node sequential_3/embedding_3/embedding_lookup}}]] [Op:__inference_train_function_7545]"
     ]
    }
   ],
   "source": [
    "print('start')\n",
    "# 데이터는 그대로고 모델만 수정해서 확인할 경우\n",
    "# 중복으로 읽고 전처리하는 대신 FIXED 변수를 통해 제어하세요.\n",
    "# 'df' in globals() 은 변수의 존재 여부를 판단합니다.\n",
    "FIXED = True\n",
    "if not FIXED and 'df' in not globals():\n",
    "    print('get data')\n",
    "    df = pd.read_csv('train.csv')\n",
    "\n",
    "    if config['is_preprocess']:\n",
    "        print('preprocess')\n",
    "        df = preprocess(df)\n",
    "else:\n",
    "    print('data was fixed')\n",
    "\n",
    "print('start train')\n",
    "if config['is_dnn']:\n",
    "    print('select dnn')\n",
    "    # 가정: 입력 크기는 1000, 출력 클래스는 2\n",
    "    max_words = 1000\n",
    "    output_dim = 4\n",
    "\n",
    "    print('tokenize')\n",
    "    # Tokenizer를 생성하고 텍스트 데이터에 적합시킵니다.\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(df['conversation'])\n",
    "\n",
    "    # 텍스트를 정수 인덱스 시퀀스로 변환합니다.\n",
    "    sequences = tokenizer.texts_to_sequences(df['conversation'])\n",
    "\n",
    "    # 시퀀스의 길이를 맞추기 위해 패딩을 추가합니다.\n",
    "    data_pad = pad_sequences(sequences, padding='post')\n",
    "\n",
    "    # class 열을 숫자로 변환\n",
    "    encoder = LabelEncoder()\n",
    "    df['class'] = encoder.fit_transform(df['class'])\n",
    "\n",
    "    print('split dataset')\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_pad, df['class'], test_size=0.2)\n",
    "\n",
    "    # word2vec\n",
    "    assert not (config['is_word2vec'] and config['is_fasttext']), 'you have to choose only one among word2vec and fasttext'\n",
    "    if config['is_word2vec'] or config['is_fasttext']:\n",
    "        word_index = tokenizer.word_index\n",
    "        index_word = {idx: word for word, idx in word_index.items()}\n",
    "        recovered = [[index_word[idx] for idx in seq if idx != 0] for seq in sequences]\n",
    "        \n",
    "        if config['is_word2vec']:\n",
    "            print('select word2vec')\n",
    "            # Word2Vec 모델 학습\n",
    "            word2vec_model = Word2Vec(sentences=recovered, \n",
    "                                    size=100, \n",
    "                                    window=5, \n",
    "                                    min_count=1, \n",
    "                                    workers=4,\n",
    "                                    sg=0)\n",
    "\n",
    "            # 단어 인덱스와 임베딩 매트릭스 생성\n",
    "            vocab_size = len(word2vec_model.wv.vocab) + 1  # +1 for padding\n",
    "            embedding_dim = word2vec_model.wv.vector_size\n",
    "            embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "            for i, word in enumerate(word2vec_model.wv.vocab):\n",
    "                embedding_matrix[i] = word2vec_model.wv[word]\n",
    "\n",
    "            # Embedding layer with pre-trained Word2Vec weights\n",
    "            embedding_layer = Embedding(vocab_size, \n",
    "                                        embedding_dim, \n",
    "                                        weights=[embedding_matrix], \n",
    "                                        input_length=244,  # 뭐 차원이 다르다 이러면 요거 건드려 보세요.\n",
    "                                        trainable=False)  # Keep embeddings fixed\n",
    "        elif config['is_fasttext']:\n",
    "            print('select fasttext')\n",
    "            fasttext_model = FastText(\n",
    "                sentences=recovered,\n",
    "                window=5,\n",
    "                min_count=5,\n",
    "                workers=4,\n",
    "                sg=1\n",
    "            )\n",
    "\n",
    "            embedding_dim = fasttext_model.vector_size\n",
    "            vocab_size = len(fasttext_model.wv.vocab)\n",
    "            embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "            for i, word in enumerate(fasttext_model.wv.vocab):\n",
    "                embedding_matrix[i] = fasttext_model.wv[word]\n",
    "\n",
    "            embedding_layer = Embedding(vocab_size,\n",
    "                                        embedding_dim,\n",
    "                                        weights=[embedding_matrix],\n",
    "                                        input_length=244,\n",
    "                                        trainable=False)\n",
    "\n",
    "    else:\n",
    "        print('select normal')\n",
    "        embedding_layer = Embedding(len(tokenizer.word_index)+1, 128, input_length=data_pad.shape[1])\n",
    "\n",
    "    print('create model')\n",
    "    # 모델을 생성합니다.\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(len(df['class'].unique()), activation='softmax'))\n",
    "\n",
    "    # 모델을 컴파일합니다.\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print('model fit')\n",
    "    # 모델을 학습합니다.\n",
    "    model.fit(x_train, y_train, epochs=10)\n",
    "\n",
    "    print('model evaluate')\n",
    "    model.evaluate(x=x_test, y=y_test)\n",
    "else:\n",
    "    print('select ml')\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    x_train = vectorizer.fit_transform(df['conversation'])\n",
    "    y_train = df['class']\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2)\n",
    "\n",
    "\n",
    "    model = MultinomialNB()\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    score = model.score(x_test, y_test)\n",
    "    print('Accuracy:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "필요시 참고 링크\n",
    "KoEDA\n",
    "https://github.com/toriving/KoEDA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
