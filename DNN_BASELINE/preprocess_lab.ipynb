{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cddb3df",
   "metadata": {},
   "source": [
    "### 머신러닝 MultinomialNB test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f9b9719",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T17:07:58.340351600Z",
     "start_time": "2023-07-09T17:07:56.251535200Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB #다항분포 나이브 베이즈 모델\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from konlpy.tag import Mecab, Okt, Kkma\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding\n",
    "from gensim.models import Word2Vec\n",
    "try:\n",
    "    from koeda import AEDA, EDA, RD, RI, SR, RS\n",
    "except ImportError:\n",
    "    !pip install koeda\n",
    "    from koeda import AEDA, EDA, RD, RI, SR, RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T17:07:58.869212100Z",
     "start_time": "2023-07-09T17:07:58.854447900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    def _aug_setup():\n",
    "        global config\n",
    "\n",
    "        if config['aug']['mode'] == 'e':\n",
    "            augmenter = EDA(morpheme_analyzer=config['morp'], alpha_sr=config['aug']['sr']['a'], alpha_ri=config['aug']['ri']['a'], alpha_rs=config['aug']['rs']['a'], prob_rd=config['aug']['rd']['a'])\n",
    "            p = (config['aug']['sr']['p'], config['aug']['ri']['p'], config['aug']['rs']['p'], config['aug']['rd']['p'])\n",
    "        elif config['aug']['mode'] == 'a':\n",
    "            augmenter = AEDA(morpheme_analyzer=config['morp'], punc_ratio=0.3)\n",
    "            p = max(config['aug']['sr']['p'], config['aug']['ri']['p'], config['aug']['rs']['p'], config['aug']['rd']['p'])\n",
    "        else:\n",
    "            augmenter = []\n",
    "            if config['aug']['rd']['p'] != 0.0:\n",
    "                augmenter.append(\n",
    "                    (RD(morpheme_analyzer=config['morp']), config['aug']['rd']['p'])\n",
    "                )\n",
    "\n",
    "            if config['aug']['ri']['p'] != 0.0:\n",
    "                augmenter.append(\n",
    "                    (RI(morpheme_analyzer=config['morp'], stopword=config['aug']['stopword']), config['aug']['ri']['p'])\n",
    "                )\n",
    "\n",
    "            if config['aug']['sr']['p'] != 0.0:\n",
    "                augmenter.append(\n",
    "                    (SR(morpheme_analyzer=config['morp'], stopword=config['aug']['stopword']), config['aug']['sr']['p'])\n",
    "                )\n",
    "\n",
    "            if config['aug']['rs']['p'] != 0.0:\n",
    "                augmenter.append(\n",
    "                    (RS(morpheme_analyzer=config['morp']), config['aug']['rs']['p'])\n",
    "                )\n",
    "            p = None\n",
    "\n",
    "        return augmenter, p\n",
    "\n",
    "    def _aug(text):\n",
    "        global config\n",
    "        nonlocal augmenter\n",
    "        nonlocal p\n",
    "\n",
    "        if isinstance(augmenter, list):\n",
    "            result = text\n",
    "\n",
    "            for aug, p in augmenter:\n",
    "                result = aug(result, p, config['aug']['repetition'])\n",
    "        else:\n",
    "            result = augmenter(text, p, config['aug']['repetition'])\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    global config\n",
    "\n",
    "    if config['is_cut']:\n",
    "        # 'conversation' 열의 각 항목에 대한 문자 수를 계산합니다.\n",
    "        data['conversation_length'] = data['conversation'].apply(len)\n",
    "\n",
    "        # 문자 수가 400 미만인 행만 선택합니다.\n",
    "        data = data[data['conversation_length'] < config['cut_point']]\n",
    "\n",
    "    if config['is_aug']:\n",
    "        # 중복 augmenter 생성 방지를 위해서 처음 한번에 생성\n",
    "        augmenter, p = _aug_setup()\n",
    "        \n",
    "        # 랜덤하게 행 선택 (예: 전체 행의 20%를 선택)\n",
    "        random_indices = np.random.choice(data.index, size=int(len(data) * config['aug']['ratio']), replace=False)\n",
    "\n",
    "        # 선택된 행에 대해 Random swap 함수 적용\n",
    "        augmented_rows = data.loc[random_indices, 'conversation'].apply(_aug)\n",
    "\n",
    "        # 증강된 데이터를 복사하고, 'text' 열에 증강된 텍스트를 삽입\n",
    "        new_rows = data.loc[random_indices].copy()\n",
    "        new_rows['conversation'] = augmented_rows\n",
    "\n",
    "        # 원본 데이터프레임에 증강된 데이터 추가\n",
    "        data = pd.concat([data, new_rows])\n",
    "    else:\n",
    "        data['conversation'] = data['conversation'].apply(config['morp'].morphs)\n",
    "\n",
    "    if config['is_stopword']:\n",
    "        pass\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T17:17:36.270882100Z",
     "start_time": "2023-07-09T17:17:36.261419600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'is_preprocess': True, # 전처리 여부\n",
    "    'is_dnn': True, # 딥러닝 사용 여부\n",
    "    'morp': Kkma(), # 형태소 분석기 변경,\n",
    "    'is_aug': False, # 데이터 augment 사용 여부\n",
    "    # 데이터 augmentation 파라미터의 경우 아래의 글들을 참고하면 좋음\n",
    "    # https://yeon22.tistory.com/203\n",
    "    # https://catsirup.github.io/ai/2020/04/21/nlp_data_argumentation.html\n",
    "    'aug': {\n",
    "        'ratio': 0.3, # 적용할 데이터의 비율\n",
    "        # 사용하기 싫은 것은 p의 값을 0.0으로 지정 한다.\n",
    "        'rd': {\n",
    "            # 여기서 a는 사실 확률이다.\n",
    "            # API 통일을 위해 a로 표기했지만, prob_rd가 본명칭\n",
    "            'a': 0.3,\n",
    "            'p': 0.4,\n",
    "        }, # RandomDeletion\n",
    "        'ri': {\n",
    "            'a': 0.3, # alpha 값이고 데이터 증강 기법의 강도를 의미한다.\n",
    "            'p': 0.4, # p는 증강 기법이 \"얼마나 자주\" 적용될지\n",
    "        }, # RandomInsertion\n",
    "        'sr': {\n",
    "            'a': 0.3,\n",
    "            'p': 0.4,\n",
    "        }, # SynonymReplacement\n",
    "        'rs': {\n",
    "            'a': 0.3,\n",
    "            'p': 0.4,\n",
    "        }, # RandomSwap\n",
    "        'mode': 'e', # EDA: e, AEDA: a, other: o\n",
    "        'stopword': True,\n",
    "        'repetition': 1 # 반복 여부인 것 같아요.\n",
    "    },\n",
    "    'is_cut': True, # 단어길이 자를건지\n",
    "    'cut_point': 400, # 자르는 기준\n",
    "    'is_word2vec': True, # word2vec 사용 여부\n",
    "    'is_stopword': False # syh님\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25221c0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T17:30:46.229613400Z",
     "start_time": "2023-07-09T17:17:39.573110900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "get data\n",
      "preprocess\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tama0\\AppData\\Local\\Temp\\ipykernel_8704\\436782421.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['conversation'] = data['conversation'].apply(config['morp'].morphs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train\n",
      "select dnn\n",
      "tokenize\n",
      "split dataset\n",
      "100\n",
      "create model\n",
      "model fit\n",
      "Epoch 1/10\n",
      "93/93 [==============================] - 2s 12ms/step - loss: 2.0703 - accuracy: 0.2756\n",
      "Epoch 2/10\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 1.2653 - accuracy: 0.3708\n",
      "Epoch 3/10\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 1.1612 - accuracy: 0.4555\n",
      "Epoch 4/10\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 1.0274 - accuracy: 0.5550\n",
      "Epoch 5/10\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.9132 - accuracy: 0.6184\n",
      "Epoch 6/10\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.7752 - accuracy: 0.6926\n",
      "Epoch 7/10\n",
      "93/93 [==============================] - 1s 9ms/step - loss: 0.6684 - accuracy: 0.7460\n",
      "Epoch 8/10\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.5500 - accuracy: 0.8070\n",
      "Epoch 9/10\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.4488 - accuracy: 0.8546\n",
      "Epoch 10/10\n",
      "93/93 [==============================] - 1s 10ms/step - loss: 0.3434 - accuracy: 0.9076\n",
      "model evaluate\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 1.9261 - accuracy: 0.4130\n"
     ]
    }
   ],
   "source": [
    "print('start')\n",
    "# 데이터는 그대로고 모델만 수정해서 확인할 경우\n",
    "# 중복으로 읽고 전처리하는 대신 FIXED 변수를 통해 제어하세요.\n",
    "# 'df' in globals() 은 변수의 존재 여부를 판단합니다.\n",
    "FIXED = False\n",
    "if not FIXED and 'df' in globals():\n",
    "    print('get data')\n",
    "    df = pd.read_csv('train.csv')\n",
    "\n",
    "    if config['is_preprocess']:\n",
    "        print('preprocess')\n",
    "        df = preprocess(df)\n",
    "else:\n",
    "    print('data was fixed')\n",
    "\n",
    "print('start train')\n",
    "if config['is_dnn']:\n",
    "    print('select dnn')\n",
    "    # 가정: 입력 크기는 1000, 출력 클래스는 2\n",
    "    max_words = 1000\n",
    "    output_dim = 4\n",
    "\n",
    "    print('tokenize')\n",
    "    # Tokenizer를 생성하고 텍스트 데이터에 적합시킵니다.\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(df['conversation'])\n",
    "\n",
    "    # 텍스트를 정수 인덱스 시퀀스로 변환합니다.\n",
    "    sequences = tokenizer.texts_to_sequences(df['conversation'])\n",
    "\n",
    "    # 시퀀스의 길이를 맞추기 위해 패딩을 추가합니다.\n",
    "    data_pad = pad_sequences(sequences, padding='post')\n",
    "\n",
    "    # class 열을 숫자로 변환\n",
    "    encoder = LabelEncoder()\n",
    "    df['class'] = encoder.fit_transform(df['class'])\n",
    "\n",
    "    print('split dataset')\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_pad, df['class'], test_size=0.2)\n",
    "\n",
    "    # word2vec\n",
    "    if config['is_word2vec']:\n",
    "        # Word2Vec 모델 학습\n",
    "        word2vec_model = Word2Vec(sentences=df['conversation'], \n",
    "                                  size=100, \n",
    "                                  window=5, \n",
    "                                  min_count=1, \n",
    "                                  workers=4,\n",
    "                                  sg=0)\n",
    "\n",
    "        # 단어 인덱스와 임베딩 매트릭스 생성\n",
    "        vocab_size = len(word2vec_model.wv.vocab) + 1  # +1 for padding\n",
    "        embedding_dim = word2vec_model.wv.vector_size\n",
    "        embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "        for i, word in enumerate(word2vec_model.wv.vocab):\n",
    "            embedding_matrix[i] = word2vec_model.wv[word]\n",
    "\n",
    "        print(embedding_dim)\n",
    "        # Embedding layer with pre-trained Word2Vec weights\n",
    "        embedding_layer = Embedding(vocab_size, \n",
    "                                    embedding_dim, \n",
    "                                    weights=[embedding_matrix], \n",
    "                                    input_length=244,  # 뭐 차원이 다르다 이러면 요거 건드려 보세요.\n",
    "                                    trainable=False)  # Keep embeddings fixed\n",
    "\n",
    "    else:\n",
    "        embedding_layer = Embedding(len(tokenizer.word_index)+1, 128, input_length=data_pad.shape[1])\n",
    "\n",
    "    print('create model')\n",
    "    # 모델을 생성합니다.\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(len(df['class'].unique()), activation='softmax'))\n",
    "\n",
    "    # 모델을 컴파일합니다.\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "    print('model fit')\n",
    "    # 모델을 학습합니다.\n",
    "    model.fit(x_train, y_train, epochs=10)\n",
    "\n",
    "    print('model evaluate')\n",
    "    model.evaluate(x=x_test, y=y_test)\n",
    "else:\n",
    "    print('select ml')\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    x_train = vectorizer.fit_transform(df['conversation'])\n",
    "    y_train = df['class']\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2)\n",
    "\n",
    "\n",
    "    model = MultinomialNB()\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    score = model.score(x_test, y_test)\n",
    "    print('Accuracy:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "필요시 참고 링크\n",
    "KoEDA\n",
    "https://github.com/toriving/KoEDA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
