{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !pip install tensorflow-gpu==2.10 pandas numpy matplotlib scikit-learn koeda konlpy gensim==3.8 jupyter\n",
    " # pip install autokeras keras-tuner\n",
    "\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install pandas numpy matplotlib scikit-learn koeda gensim==3.8 jupyter\n",
    "!pip install transformers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "## GPU 확인\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device count: 1\n",
      "0 : NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "n_devices = torch.cuda.device_count()\n",
    "print('device count:',n_devices)\n",
    "\n",
    "for i in range(n_devices):\n",
    "    print(i,':',torch.cuda.get_device_name(i))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "['[CLS]지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 거면 혼자 죽지 우리까지 사건에 휘말리게 해? 진짜 죽여버리고 싶게.\\n 정말 잘못했습니다.\\n 너가 선택해. 너가 죽을래 네 가족을 죽여줄까.\\n 죄송합니다. 정말 잘못했습니다.\\n 너에게는 선택권이 없어. 선택 못한다면 너와 네 가족까지 모조리 죽여버릴거야.\\n 선택 못하겠습니다. 한번만 도와주세요.\\n 그냥 다 죽여버려야겠군. 이의 없지?\\n 제발 도와주세요.[SEP]',\n '[CLS]길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 한번만 더 얘기한다.\\n장난전화 걸지 마시죠. \\n9시 40분 마트에 폭발물이 터지면 다 죽는거야. \\n장난전화는 업무방해죄에 해당됩니다.\\n판단은 너에게 달려있다. 길동경찰서에도 폭발물 터지면 꽤나 재미있겠지.\\n선생님 진정하세요.\\n난 이야기했어. 경고했다는 말이야.[SEP]',\n '[CLS]너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미없어.\\n지영아 너가 키 160이지? 그럼 재는 160도 안돼는거네?\\n너 군대도 안가고 좋겠다.\\n니들이 나 작은데 보태준거 있냐?\\n난쟁이들도 장가가고하던데. 너도 희망을 가져봐 \\n더이상 하지마라. \\n그 키크는 수술도 있대잖아? 니네 엄마는 그거 안해주디?\\n나람 해줬어. 저 키로 어찌살아.\\n제발 그만 괴롭히라고![SEP]',\n '[CLS]어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?\\n얘 돈 좀 있나봐\\n아니에요.돈 없어요\\n뒤져서 나오면 넌 죽는다\\n오늘 피시방 콜?\\n콜. 마지막 기회다. 있는거 다 내놔\\n정말 없어요[SEP]',\n '[CLS]저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 발라보실래요?\\n아 진짜요? 안 그래도 선크림 필요해서 알아보던 중인데 한 번 발라 볼게요!\\n여기 한 번 발라보세요. 진짜 성분도 좋고 다 좋아요.\\n음. 성분이 좋다고 하셔서 좋은거 같기는 한데 제 피부에 맞지 않나봐요. 피부가 따끔거리네요.\\n이번에 진짜 열심히 연구해서 만든건데 피부가 많이 예민하신가봐요.\\n네 많이 예민해요. 그럼 많이 파시고 안녕히 계세요.\\n아니 저기요 돈 안내요?\\n네? 발라보는것도 돈 내야 하나요?\\n그럼 이거 누구한테 팔아요? 당신이 바른거를?\\n아니 먼저 발라 보시라고 하셨잖아요. 먼저 권유해놓고 사라고 강매하는거 갈취인거 몰라요?\\n내가 안 사도 된다고 말 한 적 있어? 그것도 모르고 바른걸 누구 탓 하나? 빨리 사 당신이 바른거 당신이 사야지\\n진짜 어이가 없어서 다른 사람들한텐 이렇게 갈취하지마세요. 화딱지나네[SEP]']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../train.csv')\n",
    "document_bert = [\"[CLS]\"+str(s)+\"[SEP]\" for s in data.conversation]\n",
    "document_bert[:5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['[CLS]',\n '지',\n '##금',\n '너',\n '스',\n '##스로',\n '##를',\n '죽',\n '##여',\n '##달',\n '##라고',\n '애',\n '##원',\n '##하는',\n '것',\n '##인',\n '##가',\n '?',\n '[UNK]',\n '.',\n '죄',\n '##송',\n '##합',\n '##니다',\n '.',\n '죽',\n '##을',\n '거',\n '##면',\n '혼',\n '##자',\n '죽',\n '##지',\n '우',\n '##리',\n '##까지',\n '사건',\n '##에',\n '휘',\n '##말',\n '##리',\n '##게',\n '해',\n '?',\n '진',\n '##짜',\n '죽',\n '##여',\n '##버',\n '##리고',\n '싶',\n '##게',\n '.',\n '정',\n '##말',\n '잘',\n '##못',\n '##했',\n '##습',\n '##니다',\n '.',\n '너',\n '##가',\n '선',\n '##택',\n '##해',\n '.',\n '너',\n '##가',\n '죽',\n '##을',\n '##래',\n '네',\n '가',\n '##족',\n '##을',\n '죽',\n '##여',\n '##줄',\n '##까',\n '.',\n '죄',\n '##송',\n '##합',\n '##니다',\n '.',\n '정',\n '##말',\n '잘',\n '##못',\n '##했',\n '##습',\n '##니다',\n '.',\n '너',\n '##에게',\n '##는',\n '선',\n '##택',\n '##권',\n '##이',\n '없',\n '##어',\n '.',\n '선',\n '##택',\n '못',\n '##한다',\n '##면',\n '너',\n '##와',\n '네',\n '가',\n '##족',\n '##까지',\n '모',\n '##조',\n '##리',\n '죽',\n '##여',\n '##버',\n '##릴',\n '##거',\n '##야',\n '.',\n '선',\n '##택',\n '못',\n '##하',\n '##겠',\n '##습',\n '##니다',\n '.',\n '한',\n '##번',\n '##만',\n '도',\n '##와',\n '##주',\n '##세',\n '##요',\n '.',\n '그',\n '##냥',\n '다',\n '죽',\n '##여',\n '##버',\n '##려',\n '##야',\n '##겠',\n '##군',\n '.',\n '이',\n '##의',\n '없',\n '##지',\n '?',\n '제',\n '##발',\n '도',\n '##와',\n '##주',\n '##세',\n '##요',\n '.',\n '[SEP]']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased',do_lower_case=False)\n",
    "tokenized_texts = [ tokenizer.tokenize(s) for s in document_bert]\n",
    "tokenized_texts[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "874\n"
     ]
    }
   ],
   "source": [
    "# 'conversation' 열에서 각각의 문장의 길이를 구합니다.\n",
    "data['conversation_length'] = data['conversation'].apply(lambda x: len(x))\n",
    "\n",
    "# 가장 긴 문장의 길이를 찾습니다.\n",
    "max_conversation_length = data['conversation_length'].max()\n",
    "\n",
    "print(max_conversation_length)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding\n",
      "input_ids[0]: [   101   9706  40032   9004   9477  94980  11513   9690  29935  89851\n",
      "  59894   9532  14279  12178   8870  12030  11287    136    100    119\n",
      "   9687 119057  33188  48345    119   9690  10622   8863  14867   9987\n",
      "  13764   9690  12508   9604  12692  18382  97435  10530  10010  89523\n",
      "  12692  14153   9960    136   9708 119235   9690  29935  41605  62211\n",
      "   9495  14153    119   9670  89523   9654 118940 119424 119081  48345\n",
      "    119   9004  11287   9428 119342  14523    119   9004  11287   9690\n",
      "  10622  37388   9011   8843  52560  10622   9690  29935 119219 118671\n",
      "    119   9687 119057  33188  48345    119   9670  89523   9654 118940\n",
      " 119424 119081  48345    119   9004  26212  11018   9428 119342  25347\n",
      "  10739   9555  12965    119   9428 119342   9290  14102  14867   9004\n",
      "  12638   9011   8843  52560  18382   9283  20626  12692   9690  29935\n",
      "  41605  85836  41521  21711    119   9428 119342   9290  35506 118632\n",
      " 119081  48345    119   9954  35465  19105   9087  12638  16323  24982\n",
      "  48549    119   8924 118729   9056   9690  29935  41605  26737  21711\n",
      " 118632  17360    119   9638  10459   9555  12508    136   9672  51431\n",
      "   9087  12638  16323  24982  48549    119    102      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0]\n"
     ]
    }
   ],
   "source": [
    "print('padding')\n",
    "MAX_LEN = 500\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen = MAX_LEN, dtype='long', truncating='post', padding='post')\n",
    "input_ids[0]\n",
    "print('input_ids[0]:',input_ids[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_masks[0]: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "#학습 속도를 높이기 위해 실 데이터가 있는 곳과 padding이 있는곳을 attention에게 알려줌\n",
    "\n",
    "attention_masks = []\n",
    "\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "print('attention_masks[0]:',attention_masks[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "data['class'] = data['class'].factorize()[0]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split train - val\n"
     ]
    }
   ],
   "source": [
    "# input 과 mask가 뒤섞이지 않도록 random_state를 일정하게 고정.\n",
    "# test set은 위에서 분리되었기에 , train 과 validation set만 분리\n",
    "print('split train - val')\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = \\\n",
    "train_test_split(input_ids, data['class'].values, random_state=42, test_size=0.1)\n",
    "\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=42,test_size=0.1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert data to tenser..\n"
     ]
    }
   ],
   "source": [
    "# numpy ndarray로 되어있는 input,lable,mask들을 torch tensor로 변환\n",
    "print('convert data to tenser..')\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set batch and data loader\n"
     ]
    }
   ],
   "source": [
    "#현재 쓰고있는 GPU의 VRAM에 맞게 배치사이즈 설정(크게 설정후 부족메시지가 뜨면 8의 배수중 작은것으로 줄여나가기)\n",
    "print('set batch and data loader')\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = BATCH_SIZE)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "#GPU 체크 및 할당\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making BERT model for classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=4, bias=True)\n)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('making BERT model for classification')\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=4)\n",
    "model.cuda()\n",
    "# model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schedule start\n",
      "train start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tama0\\anaconda3\\envs\\tf210\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# transformers에서 제공하는 옵티마이저 중 AdamW를 사용\n",
    "# 총 훈련 스텝은 이터레이션 * 에폭 수로 설정\n",
    "# 러닝 레잇 스케쥴러도 transformers에서 제공하는것을 사용\n",
    "print('schedule start')\n",
    "#옵티마이저 설정\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # 학습률\n",
    "                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n",
    "                )\n",
    "\n",
    "# 에폭수\n",
    "epochs = 4\n",
    "\n",
    "# 총 훈련 스텝\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# lr 조금씩 감소시키는 스케줄러\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "##학습\n",
    "# accuracy 와 시간 표시함수 정의\n",
    "# 정확도 계산 함수\n",
    "print('train start')\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# 시간 표시 함수\n",
    "def format_time(elapsed):\n",
    "    # 반올림\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # hh:mm:ss으로 형태 변경\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([3, 500])\n",
      "\n",
      "\n",
      "\n",
      "  Average training loss: 1.06\n",
      "  Training epcoh took: 0:16:37\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.77\n",
      "  Validation took: 0:00:40\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([32, 500])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# gradient update는 명시적으로 하지 않고 위에서 로드한 optimizer를 활용\n",
    "# 재현을 위해 랜덤시드 고정\n",
    "# 모든 Epoch를 학습하면 학습이 종료\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# 그래디언트 초기화\n",
    "model.zero_grad()\n",
    "\n",
    "# 에폭만큼 반복\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # 시작 시간 설정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 로스 초기화\n",
    "    total_loss = 0\n",
    "\n",
    "    # 훈련모드로 변경\n",
    "    model.train()\n",
    "\n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # 경과 정보 표시\n",
    "        if step % 500 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        print(f'\\n\\n{b_input_ids.shape}\\n\\n')\n",
    "\n",
    "        # Forward 수행\n",
    "        outputs = model(b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask,\n",
    "                        labels=b_labels)\n",
    "\n",
    "        # 로스 구함\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # 총 로스 계산\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward 수행으로 그래디언트 계산\n",
    "        loss.backward()\n",
    "\n",
    "        # 그래디언트 클리핑\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 그래디언트를 통해 가중치 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 스케줄러로 학습률 감소\n",
    "        scheduler.step()\n",
    "\n",
    "        # 그래디언트 초기화\n",
    "        model.zero_grad()\n",
    "\n",
    "    # 평균 로스 계산\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    #시작 시간 설정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 평가모드로 변경\n",
    "    model.eval()\n",
    "\n",
    "    # 변수 초기화\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for batch in validation_dataloader:\n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # 그래디언트 계산 안함\n",
    "        with torch.no_grad():\n",
    "            # Forward 수행\n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask)\n",
    "\n",
    "        # 로스 구함\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # CPU로 데이터 이동\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
